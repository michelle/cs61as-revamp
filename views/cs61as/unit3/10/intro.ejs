<!-- Introduction 10: Client/server and Concurrency -->

<p>
We've seen the power of computational objects with local state as
tools for modeling.  Yet, as section&nbsp;3.1.3
warned, this power extracts a price: the loss of referential
transparency, giving rise to a thicket of questions about sameness and
change, and the need to abandon the substitution model of evaluation in
favor of the more intricate environment model.<p> 
 
<a name="%_idx_3580"></a>The central issue lurking beneath the complexity of state, sameness,
and change is that by introducing assignment we are forced to admit
<em>time</em> into our computational models.  Before we introduced
assignment, all our programs were timeless, in the sense that any
expression that has a value always has the same value.  In contrast,
recall the example of modeling withdrawals from a bank account
and returning the resulting balance,
introduced at the beginning of
section&nbsp;3.1.1:<p> 
 
<p><p><tt>(withdraw&nbsp;25)<br> 
<i>75</i><br> 
(withdraw&nbsp;25)<br> 
<i>50</i><br> 
</tt><p><p> 
Here successive evaluations of the same expression yield different
values.  This behavior arises from the fact that the execution of
assignment statements (in this case, assignments to the variable <tt>balance</tt>) delineates <em>moments in time</em> when values change.  The
result of evaluating an expression depends not only on the expression
itself, but also on whether the evaluation occurs before or after
these moments.  Building models in terms of computational objects with
local state forces us to confront time as an essential concept in
programming.<p> 
 
We can go further in structuring computational models to match our perception
of the physical world.  Objects in the world do not change one at a time in
sequence.  Rather we perceive them as acting <em>concurrently</em>&mdash;all
at once.  So it is often natural to model systems as collections of
computational processes that execute concurrently.  Just as we can make our
programs modular by organizing models in terms of objects with separate local
state, it is often appropriate to divide computational models into parts that
evolve separately and concurrently.  Even if the programs are to be executed
on a sequential computer, the practice of writing programs as if they were to
be executed concurrently forces the programmer to avoid inessential timing
constraints and thus makes programs more modular.<p>
 
In addition to making programs more modular, concurrent computation
can provide a speed advantage over sequential computation.  Sequential
computers execute only one operation at a time, so the amount of time
it takes to perform a task is proportional to the total number of
operations performed. 
However, if it is possible to decompose a problem into pieces that are
relatively independent and need to communicate only rarely, it may be
possible to allocate pieces to separate computing processors,
producing a speed advantage proportional to the number of processors
available.<p> 
 
Unfortunately, the complexities introduced by assignment become even
more problematic in the presence of concurrency.  The fact of
concurrent execution, either because the world operates in parallel or
because our computers do, entails additional complexity in our
understanding of time.
<p></p>
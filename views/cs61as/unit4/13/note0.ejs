<!-- Lecture Notes 13: Analyzing evaluator, MapReduce -->

<p>
<b>Reading:</b>
Abelson &amp; Sussman, Section
<a href="http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1.7http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1.7">4.1.7</a>,
<a href="http://inst.eecs.berkeley.edu/~cs61as/reader/mapreduce-osdi04.pdf">MapReduce paper</a>
<p>

To work with the ideas in this section you should first

<p>
<tt>  <pre>(load "&#126;cs61as/lib/analyze.scm")
</pre></tt>
in order to get the analyzing metacircular evaluator.

<p>
<b>Inefficiency in the Metacircular Evaluator</b>

<p>
Suppose we've defined the factorial function as follows:

<p>
<tt>  <pre>(define (fact num)
  (if (= num 0)
      1
      (* num (fact (- num 1)))))
</pre></tt>

<p>
What happens when we compute <tt>(fact&nbsp;3)</tt>?

<p>
<tt>  <pre>eval (fact 3)
  self-evaluating?  ==&#62;  #f     if-alternative  ==&#62;  (* num (fact (- num 1)))
  variable?  ==&#62;  #f               eval (* num (fact (- num 1)))
  quoted?  ==&#62;  #f                   self-evaluating?  ==&#62;  #f
  assignment?  ==&#62;  #f               ...
  definition?  ==&#62;  #f               list-of-values (num (fact (- num 1)))
  if?  ==&#62;  #f                         ...
  lambda?  ==&#62;  #f                     eval (fact (- num 1))
  begin?  ==&#62;  #f                        ...
  cond?  ==&#62;  #f                         apply &lt;procedure fact&#62; (2)
  application?  ==&#62;  #t                    eval (if (= num 0) ...)
  eval fact
    self-evaluating?  ==&#62;  #f
    variable?  ==&#62;  #t
    lookup-variable-value  ==&#62;  &lt;procedure fact&#62;
    list-of-values (3)
      eval 3  ==&#62; 3
    apply &lt;procedure fact&#62; (3)
      eval (if (= num 0) ...)
	self-evaluating?  ==&#62;  #f
	variable?  ==&#62;  #f
	quoted?  ==&#62;  #f
	assignment?  ==&#62;  #f
	definition?  ==&#62;  #f
        if?  ==&#62;  #t
          eval-if (if (= num 0) ...)
            if-predicate  ==&#62;  (= num 0)
              eval (= num 0)
                self-evaluating?  ==&#62;  #f
                ...
</pre></tt>

<p>
Four separate times, the evaluator has to examine the procedure body,
decide that it's an <tt>if</tt> expression, pull out its component parts,
and evaluate those parts (which in turn involves deciding what type
of expression each part is).

<p>
This is one reason why interpreted languages are so much slower than compiled
languages:  The interpreter does the syntactic analysis of the program over
and over again.  The compiler does the analysis once, and the compiled
program can just do the part of the computation that depends on the actual
values of variables.

<p>
<b>Separating Analysis from Execution</b>

<p>
<tt>Eval</tt> takes two arguments, an expression and an environment.  Of those,
the expression argument is (obviously!)&nbsp;the same every time we revisit the
same expression, whereas the environment will be different each time.  For
example, when we compute <tt>(fact&nbsp;3)</tt> we evaluate the body of <tt>fact</tt>
in an environment in which <tt>num</tt> has the value 3.  That body includes
a recursive call to compute <tt>(fact&nbsp;2)</tt>, in which we evaluate the same
body, but now in an environment with <tt>num</tt> bound to 2.

<p>
Our plan is to look at the evaluation process, find those parts which depend
only on <tt>exp</tt> and not on <tt>env</tt>, and do those only once.  The
procedure that does this work is called <tt>analyze</tt>.

<p>
What is the result of <tt>analyze</tt>?  It has to be something that can
be combined somehow with an environment in order to return a value.
The solution is that <tt>analyze</tt> returns a procedure that takes only
<tt>env</tt> as an argument, and does the rest of the evaluation.

<p>
Instead of

<p>
<tt>  <pre>(eval exp env)  ==&#62;  value
</pre></tt>

<p>
we now have

<p>
<tt>  <pre>1.  (analyze exp)  ==&#62;  exp-procedure
2.  (exp-procedure env)  ==&#62;  value
</pre></tt>

<p>
When we evaluate the same expression again, we only have to repeat step 2.
What we're doing is akin to memoization, in that we remember the result
of a computation to avoid having to repeat it.  The difference is that
now we're remembering something that's only part of the solution to the
overall problem, instead of a complete solution.

<p>
We can duplicate the effect of the original <tt>eval</tt> this way:

<p>
<tt>  <pre>(define (eval exp env)
  ((analyze exp) env))
</pre></tt>

<p>
<b>The Implementation Details</b>

<p>
<tt>Analyze</tt> has a structure similar to that of the original eval:

<p>
<tt>  <pre>(define (eval exp env)                (define (analyze exp)
  (cond ((self-evaluating? exp)         (cond ((self-evaluating? exp)
         exp)                                  (analyze-self-eval exp))
        ((variable? exp)                      ((variable? exp)
         (lookup-var-val exp env))             (analyze-var exp))
        ...                                   ...
        ((foo? exp) (eval-foo exp env))       ((foo? exp) (analyze-foo exp))
        ...))                                 ...))
</pre></tt>

<p>
The difference is that the procedures such as <tt>eval-if</tt> that take
an expression and an environment as arguments have been replaced by
procedures such as <tt>analyze-if</tt> that take only the expression as argument.

<p>

<p>
How do these analysis procedures work?  As an intermediate step in our
understanding, here is a version of <tt>analyze-if</tt> that exactly follows
the structure of <tt>eval-if</tt> and doesn't save any time:

<p>
<tt>  <pre>(define (eval-if exp env)
  (if (true? (eval (if-predicate exp) env))
      (eval (if-consequent exp) env)
      (eval (if-alternative exp) env)))

(define (analyze-if exp)
  (lambda (env)
    (if (true? (eval (if-predicate exp) env))
	(eval (if-consequent exp) env)
	(eval (if-alternative exp) env))))
</pre></tt>

<p>
This version of <tt>analyze-if</tt> returns a procedure with <tt>env</tt> as its
argument, whose body is exactly the same as the body of the original
<tt>eval-if</tt>.  Therefore, if we do

<p>
<tt>  <pre>((analyze-if some-if-expression) some-environment)
</pre></tt>

<p>
the result will be the same as if we'd said

<p>
<tt>  <pre>(eval-if some-if-expression some-environment)
</pre></tt>

<p>
in the original metacircular evaluator.

<p>
But we'd like to improve on this first version of <tt>analyze-if</tt> because
it doesn't really avoid any work.  Each time we call the procedure that
<tt>analyze-if</tt> returns, it will do all of the work that the original
<tt>eval-if</tt> did.

<p>
The first version of <tt>analyze-if</tt> contains three calls to <tt>eval</tt>.
Each of those calls does an analysis of an expression and then a
computation of the value in the given environment.  What we'd like to
do is split each of those <tt>eval</tt> calls into its two separate parts,
and do the first part only once, not every time:

<p>
<tt>  <pre>(define (analyze-if exp)
  (let ((pproc (analyze (if-predicate exp)))
	(cproc (analyze (if-consequent exp)))
	(aproc (analyze (if-alternative exp))))
    (lambda (env)
      (if (true? (pproc env))
	  (cproc env)
	  (aproc env)))))
</pre></tt>

<p>
In this final version, the procedure returned by <tt>analyze-if</tt>
doesn't contain any analysis steps.  All of the components were
already analyzed before we call that procedure, so no further
analysis is needed.

<p>
The biggest gain in efficiency comes from the way in which <tt>lambda</tt>
expressions are handled.  In the original metacircular evaluator,
leaving out some of the data abstraction for clarity here, we have

<p>
<tt>  <pre>(define (eval-lambda exp env)
  (list 'procedure exp env))
</pre></tt>

<p>
The evaluator does essentially nothing for a <tt>lambda</tt> expression
except to remember the procedure's text and the environment in which
it was created.  But in the analyzing evaluator we analyze the body
of the procedure; what is stored as the representation of the procedure
does not include its text!  Instead, the evaluator represents a procedure
in the metacircular Scheme as a procedure in the underlying Scheme,
along with the formal parameters and the defining environment.

<p>

<p>
<b>Level Confusion</b>

<p>
The analyzing evaluator turns an expression such as

<p>
<tt>  <pre>(if A B C)
</pre></tt>

<p>
into a procedure

<p>
<tt>  <pre>(lambda (env)
  (if (A-execution-procedure env)
      (B-execution-procedure env)
      (C-execution-procedure env)))
</pre></tt>

<p>
This may seem like a step backward; we're trying to implement <tt>if</tt>
and we end up with a procedure that does an <tt>if</tt>.  Isn't this
an infinite regress?

<p>
No, it isn't.  The <tt>if</tt> in the execution procedure is handled by the
underlying Scheme, not by the metacircular Scheme.  Therefore, there's no
regress; we don't call <tt>analyze-if</tt> for that one.  Also, the <tt>if</tt> in
the underlying Scheme is much faster than having to do the syntactic
analysis for the <tt>if</tt> in the meta-Scheme.

<p>
<b>So What?</b>

<p>
The syntactic analysis of expressions is a large part of what a compiler
does.  In a sense, this analyzing evaluator is a compiler!  It compiles
Scheme into Scheme, so it's not a very useful compiler, but it's really
not that much harder to compile into something else, such as the machine
language of a particular computer.

<p>
A compiler whose structure is similar to this one is called a <i>recursive
descent</i> compiler.  Today, in practice, most compilers use a different
technique (called a stack machine) because it's possible to automate the
writing of a parser that way.  (I mentioned this earlier as an example of
data-directed programming.)  But if you're writing a parser by hand, it's
easiest to use recursive descent.

<p>

<p>

<a name="mr">

<p><b>&bull; MapReduce part 1</b>

<p>
In the past, functional programming, and higher-order functions in particular,
have been considered esoteric and unimportant by most programmers.  But the
advent of highly parallel computation is changing that, because functional
programming has the very useful property that the different pieces of a
program don't interfere with each other, so it doesn't matter in what order
they are invoked.  We're about to examine one famous example of
functional programming at work: the <tt>MapReduce</tt> programming paradigm
developed by Google that uses higher-order functions to allow a programmer to
process large amount of data in parallel on many computers.

<p>
Much of the computing done at Google consists of relatively simple algorithms
applied to massive amounts of data, such as the entire World Wide Web.  It's
routine for them to use clusters consisting of many thousands of processors,
all running the same program, with a distributed filesystem that gives each
processor local access to part of the data.

<p>
In 2003 some very clever people at Google noticed that the majority of these
computations could be viewed as a <tt>map</tt> of some function over the data
followed by an <tt>accumulate</tt> (they use the name <tt>reduce</tt>, which is a
synonym for this function) to collect the results.  Although each program was
conceptually simple, a lot of programmer effort was required to manage the
parallelism; every programmer had to worry about things like how to recover
from a processor failure (virtually certain to happen when a large computation
uses thousands of machines) during the computation.  They wrote a library
procedure named <tt>MapReduce</tt> that basically takes two functions as
arguments, a one-argument function for the <tt>map</tt> part and a two-argument
function for the <tt>accumulate</tt> part.  (The actual implementation is more
complicated, but this is the essence of it.)  Thus, only the implementors of
<tt>MapReduce</tt> itself had to worry about the parallelism, and application
programmers just have to write the two function arguments.

<p><blockquote><small>We are
indebted to Google for funding both the development of software and curriculum
for this piece of the course and the cost of the cluster of parallel
processors you'll use in the lab later.  This should not be taken as an
endorsement of Google's contemptible business practices, in which their income
depends on massive violations of their users' privacy.  Opinions expressed
are those of BH, not the Regents of the University of California.</small></blockquote>

<p>
<tt>MapReduce</tt> is a little more complicated than just

<p>
<tt>  <pre>(define (mapreduce mapper reducer base-case data)    ; Nope.
  (accumulate reducer base-case (map mapper data)))
</pre></tt>

<p>
because of the parallelism.  The input data comes in pieces; several computers
run the <tt>map</tt> part in parallel, and each of them produces some output.
These intermediate results are rearranged into groups, and each computer does
a <tt>reduce</tt> of part of the data.  The final result isn't one big list, but
separate output files for each reducing process.

<p>
To make this a little more specific, today we'll see a toy version of the
algorithm that just handles small data lists in one processor.

<p>
Data pass through the program in the form of <i>key-value pairs:</i>

<p>
<tt>  <pre>(define make-kv-pair cons)
(define kv-key car)
(define kv-value cdr)
</pre></tt>

<p>
A list of key-value pairs is called an <i>association list</i> or
<i>a-list</i> for short.  We'll see a-lists in many contexts other than
<tt>MapReduce</tt>.  Conceptually, the input to <tt>MapReduce</tt> is an a-list,
although in practice there are several a-lists, each on a different processor.

<p>
Any computation in <tt>MapReduce</tt> involves two function arguments: the
<i>mapper</i> and the <i>reducer</i>.  (Note: The Google <tt>MapReduce</tt> paper
in the course reader says "the <tt>map</tt> function" to mean the function that
the user writes, the one that's applied to each datum; this usage is confusing
since everyone else uses "<tt>map</tt>" to mean the higher-order function that
controls the invocation of the user's function, so we're calling the latter
the <i>mapper</i>:

<p>
<tt>  <pre>(map mapper data)
</pre></tt>

<p>
Similarly, we'll use <tt>reduce</tt> to refer to the higher-order function, and
<tt>reducer</tt> to mean the user's accumulation function.)

<p><center><img width=100% src="/resources/mr.jpg"></center>

<p>
The argument to the mapper is always one kv-pair.  Keys are typically used to
keep track of where the data came from.  For example, if the input consists of
a bunch of Web pages, the keys might be their URLs.  Another example we'll be
using is Project Gutenberg, an online collection of public-domain books; there
the key would be the name of a book (more precisely, the filename of the file
containing that book).  In most uses of a-lists, there will only be one
kv-pair with a given key, but that's not true here; for example, each line of
text in a book or Web page might be a datum, and every line in the input will
have the same key.

<p>
The value returned by the mapper must be <i>a list of</i> kv-pairs.  The
reason it's a list instead of a single kv-pair, as you might expect, is
twofold.  First, a single input may be split into smaller pieces; for example,
a line of text might be mapped into a separate kv-pair for each word in the
line.  Second, the mapper might return an empty list, if this particular
kv-pair shouldn't contribute to the result at all; thus, the mapper might also
be viewed as a filterer.  The mapper is not required to use the same key in
its output kv-pairs that it gets in its input kv-pair.

<p>
Since <tt>map</tt> handles each datum independently of all the others, the fact
that many <tt>map</tt>s are running in parallel doesn't affect the result; we
can model the entire process with a single <tt>map</tt> invocation.  That's not
the case with the <tt>reduce</tt> part, because the data are being combined, so
it matters which data end up on which machine.  This is where the keys are
most important.  Between the mapping and the reduction is an intermediate
step in which the kv-pairs are sorted based on the keys, and all pairs with
the same key are reduced together.  Therefore, the reducer doesn't need to
look at keys at all; its two arguments are a value and the result of the
partial accumulation of values already done.  In many cases, just as in the
accumulations we've seen earlier, the reducer will be a simple associative
and commutative operation such as <tt>+</tt>.

<p>
The overall result is an a-list, in which each key occurs only once, and the
value paired with that key is the result of the <tt>reduce</tt> invocation that
handled that key.  The keys are guaranteed to be in order. (This is the result
of the 61A version of <tt>MapReduce</tt>; the real Google software has a more
complicated interface because each computer in the cluster collects its own
<tt>reduce</tt> results, and there are many options for how the reduction tasks
are distributed among the processors.  You'll learn more details in 61C.)  So
in today's single-processor simulation, instead of talking about <tt>reduce</tt>
we'll use a higher order function called <tt>groupreduce</tt> that takes a <i>
list of a-lists</i> as argument, with each sublist having kv-pairs with the
same key, does a separate reduction for each sublist, and returns an a-list of
the results.  So a complete <tt>MapReduce</tt> operation works roughly like this:

<p>
<tt>  <pre>(define (mapreduce mapper reducer base-case data) ; handwavy approximation
  (groupreduce reducer base-case
	       (sort-into-buckets (map mapper data))))

(define (groupreduce reducer base-case buckets)
  (map (lambda (subset) (make-kv-pair
			 (kv-key (car subset))
			 (reduce reducer base-case (map kv-value subset))))
       buckets))
</pre></tt>

<p>
As a first example, we'll take some grades from various exams and add up
the grades for each student.  This example doesn't require <tt>map</tt>.  Here's
the raw data:

<p>
<tt>  <pre>(define mt1 '((cs61a-xc . 27) (cs61a-ya . 40) (cs61a-xw . 35)
	      (cs61a-xd . 38) (cs61a-yb . 29) (cs61a-xf . 32)))
(define mt2 '((cs61a-yc . 32) (cs61a-xc . 25) (cs61a-xb . 40)
	      (cs61a-xw . 27) (cs61a-yb . 30) (cs61a-ya . 40)))
(define mt3 '((cs61a-xb . 32) (cs61a-xk . 34) (cs61a-yb . 30)
	      (cs61a-ya . 40) (cs61a-xc . 28) (cs61a-xf . 33)))
</pre></tt>

<p>
Each midterm in this toy problem corresponds to the output of a parallel
<tt>map</tt> operation in a real problem.

<p>
First we combine these into one list, and use that as input to the
<tt>sortintobuckets</tt> procedure:

<p>
<tt>  <pre>&#62; (sort-into-buckets (append mt1 mt2 mt3))
(((cs61a-xb . 40) (cs61a-xb . 32))
 ((cs61a-xc . 27) (cs61a-xc . 25) (cs61a-xc . 28))
 ((cs61a-xd . 38))
 ((cs61a-xf . 32) (cs61a-xf . 33))
 ((cs61a-xk . 34))
 ((cs61a-xw . 35) (cs61a-xw . 27))
 ((cs61a-ya . 40) (cs61a-ya . 40) (cs61a-ya . 40))
 ((cs61a-yb . 29) (cs61a-yb . 30) (cs61a-yb . 30))
 ((cs61a-yc . 32)))
</pre></tt>

<p>
In the real parallel context, instead of the <tt>append</tt>, each <tt>map</tt>
process would sort its own results into the right buckets, so that too would
happen in parallel.

<p>
Now we can use <tt>groupreduce</tt> to add up the scores in each bucket
separately:

<p>
<tt>  <pre>&#62; (groupreduce + 0 (sort-into-buckets (append mt1 mt2 mt3)))
((cs61a-xb . 72) (cs61a-xc . 80) (cs61a-xd . 38) (cs61a-xf . 65)
 (cs61a-xk . 34) (cs61a-xw . 62) (cs61a-ya . 120) (cs61a-yb . 89)
 (cs61a-yc . 32))
</pre></tt>

<p>
Note that the returned list has the keys in sorted order.  This is a
consequence of the sorting done by <tt>sort-into-buckets</tt>, and also, in the
real parallel <tt>mapreduce</tt>, a consequence of the order in which keys are
assigned to processors (the "partitioning function" discussed in the <tt>
MapReduce</tt> paper).

<p>
Similarly, we could ask <i>how many</i> midterms each student took:

<p>
<tt>  <pre>&#62; (groupreduce (lambda (new old) (+ 1 old)) 0
	       (sort-into-buckets (append mt1 mt2 mt3)))
((cs61a-xb . 2) (cs61a-xc . 3) (cs61a-xd . 1) (cs61a-xf . 2) (cs61a-xk . 1)
 (cs61a-xw . 2) (cs61a-ya . 3) (cs61a-yb . 3) (cs61a-yc . 1))
</pre></tt>

<p>
We could combine these in the obvious way to get the average score per
student, for exams actually taken.

<p>
<b>Word frequency counting.</b>  A common problem is to look for commonly used
words in a document.  For starters, we'll count word frequencies in a single
sentence.  The first step is to turn the sentence into key-value pairs in
which the key is the word and the value is always 1:

<p>
<tt>  <pre>&#62; (map (lambda (wd) (list (make-kv-pair wd 1))) '(cry baby cry))
((cry . 1) (baby . 1) (cry . 1))
</pre></tt>

<p>
If we group these by key and add the values, we'll get the number of times
each word appears.

<p>
<tt>  <pre>(define (wordcounts1 sent)
  (groupreduce + 0 (sort-into-buckets (map (lambda (wd) (make-kv-pair wd 1))
					   sent))))

&#62; (wordcounts1 '(cry baby cry))
((baby . 1) (cry . 2))
</pre></tt>

<p>
Now to try the same task with (simulated) files.  When we use the real
<tt>mapreduce</tt>, it'll give us file data in the form of a key-value pair
whose key is the name of the file and whose value is a line from the file,
in the form of a sentence.  For now, we're going to simulate a file as a
list whose car is the "filename" and whose cdr is a list of sentences,
representing the lines of the file.  In other words, a file is a list whose
first element is the filename and whose remaining elements are the lines.

<p>
<tt>  <pre>(define filename car)
(define lines cdr)
</pre></tt>

<p>
Here's some data for us to play with:

<p>
<tt>  <pre>(define file1 '((please please me) (i saw her standing there) (misery)
		(anna go to him) (chains) (boys) (ask me why)
		(please please me) (love me do) (ps i love you)
		(baby its you) (do you want to know a secret)))
(define file2 '((with the beatles) (it wont be long) (all ive got to do)
		(all my loving) (dont bother me) (little child)
		(till there was you) (roll over beethoven) (hold me tight)
		(you really got a hold on me) (i wanna be your man)
		(not a second time)))
(define file3 '((a hard days night) (a hard days night) 
		(i should have known better) (if i fell)
		(im happy just to dance with you) (and i love her)
		(tell me why) (cant buy me love) (any time at all)
		(ill cry instead) (things we said today) (when i get home)
		(you cant do that) (ill be back)))
</pre></tt>

<p>
We start with a little procedure to turn a "file" into an a-list in the
form <tt>mapreduce</tt> will give us:

<p>
<tt>  <pre>(define (file-&#62;linelist file)
  (map (lambda (line) (make-kv-pair (filename file) line))
       (lines file)))

&#62; (file-&#62;linelist file1)
(((please please me) i saw her standing there)
 ((please please me) misery)
 ((please please me) anna go to him)
 ((please please me) chains)
 ((please please me) boys)
 ((please please me) ask me why)
 ((please please me) please please me)
 ((please please me) love me do)
 ((please please me) ps i love you)
 ((please please me) baby its you)
 ((please please me) do you want to know a secret))
</pre></tt>

<p>
Note that <tt>((please please me) misery)</tt> is how Scheme prints the
kv-pair <tt>((please&nbsp;please&nbsp;me)&nbsp;.&nbsp;(misery))</tt>.

<p>
Now we modify our <tt>wordcounts1</tt> procedure to accept such kv-pairs:

<p>
<tt>  <pre>(define (wordcounts files)
  (groupreduce + 0 (sort-into-buckets
		    (flatmap (lambda (kv-pair)
			       (map (lambda (wd) (make-kv-pair wd 1))
				    (kv-value kv-pair)))
			     files))))
</pre></tt>

<p>
<tt>  <pre>&#62; (wordcounts (append (file-&#62;linelist file1)
		      (file-&#62;linelist file2)
		      (file-&#62;linelist file3)))
((a . 4) (all . 3) (and . 1) (anna . 1) (any . 1) (ask . 1) (at . 1)
 (baby . 1) (back . 1) (be . 3) (beethoven . 1) (better . 1) (bother . 1)
 (boys . 1) (buy . 1) (cant . 2) (chains . 1) (child . 1) (cry . 1)
 (dance . 1) (days . 1) (do . 4) (dont . 1) (fell . 1) (get . 1) (go . 1)
 (got . 2) (happy . 1) (hard . 1) (have . 1) (her . 2) (him . 1) (hold . 2)
 (home . 1) (i . 7) (if . 1) (ill . 2) (im . 1) (instead . 1) (it . 1)
 (its . 1) (ive . 1) (just . 1) (know . 1) (known . 1) (little . 1)
 (long . 1) (love . 4) (loving . 1) (man . 1) (me . 8) (misery . 1) (my . 1)
 (night . 1) (not . 1) (on . 1) (over . 1) (please . 2) (ps . 1) (really . 1)
 (roll . 1) (said . 1) (saw . 1) (second . 1) (secret . 1) (should . 1)
 (standing . 1) (tell . 1) (that . 1) (there . 2) (things . 1) (tight . 1)
 (till . 1) (time . 2) (to . 4) (today . 1) (wanna . 1) (want . 1) (was . 1)
 (we . 1) (when . 1) (why . 2) (with . 1) (wont . 1) (you . 7) (your . 1))
</pre></tt>

<p>
(If you count yourself to check, remember that words in the album titles
don't count!  They're keys, not values.)

<p>
Note the call to <tt>flatmap</tt> above.  In a real <tt>mapreduce</tt>, each file
would be mapped on a different processor, and the results would be distributed
to <tt>reduce</tt> processes in parallel.  Here, the <tt>map</tt> over files gives
us a list of a-lists, one for each file, and we have to append them to form
a single a-list.  <tt>Flatmap</tt> flattens (appends) the results from calling
<tt>map</tt>.

<p>
We can postprocess the groupreduce output to get an overall reduction to
a single value:

<p>
<tt>  <pre>(define (mostfreq files)
  (accumulate (lambda (new old)
		(cond ((&#62; (kv-value new) (kv-value (car old)))
		       (list new))
		      ((= (kv-value new) (kv-value (car old)))
		       (cons new old))	; In case of tie, remember both.
		      (else old)))
	      (list (make-kv-pair 'foo 0))	; Starting value.
	      (groupreduce + 0 (sort-into-buckets
				(flatmap (lambda (kv-pair)
					   (map (lambda (wd)
						  (make-kv-pair wd 1))
						(kv-value kv-pair)))
					 files)))))

&#62; (mostfreq (append (file-&#62;linelist file1)
		    (file-&#62;linelist file2)
		    (file-&#62;linelist file3)))
((me . 8))
</pre></tt>

<p>
(Second place is "you" and "I" with 7 appearances each, which would have
made a two-element a-list as the result.)  If we had a truly enormous word
list, we'd put it into a distributed file and use another <tt>mapreduce</tt> to
find the most frequent words of subsets of the list, and then find the most
frequent word of those most frequent words.

<p>
<b>Searching for a pattern.</b>  Another task is to search through files for
lines matching a pattern.  A <i>pattern</i> is a sentence in which the
word <tt>*</tt> matches any set of zero or more words:

<p>
<tt>  <pre>&#62; (match? '(* i * her *) '(i saw her standing there))
#t
&#62; (match? '(* i * her *) '(and i love her))
#t
&#62; (match? '(* i * her *) '(ps i love you))
#f
</pre></tt>

<p>
Here's how we look for lines in files that match a pattern:

<p>
<tt>  <pre>(define (grep pattern files)
  (groupreduce cons '()
	       (sort-into-buckets
		(flatmap (lambda (kv-pair)
			   (if (match? pattern (kv-value kv-pair))
			       (list kv-pair)
			       '()))
			 files))))

&#62; (grep '(* i * her *) (append (file-&#62;linelist file1)
			       (file-&#62;linelist file2)
			       (file-&#62;linelist file3)))
(((a hard days night) (and i love her))
 ((please please me) (i saw her standing there)))
</pre></tt>

<p>
<b>Summary.</b>  The general pattern here is

<p>
<tt>  <pre>(groupreduce <i>reducer</i> <i>base-case</i>
	     (sort-into-buckets
	      (map-or-flatmap <i>mapper</i> <i>data</i>)))
</pre></tt>

<p>
This corresponds to

<p>
<tt>  <pre>(mapreduce <i>mapper</i> <i>reducer</i> <i>base-case</i> <i>data</i>)
</pre></tt>

<p>
in the truly parallel <tt>mapreduce</tt> exploration we'll be doing next.

<p>
<P><b>&bull; Mapreduce part 2</b>

<p>
Here's the diagram of mapreduce again:

<p><center><img width=100% src="/resources/mr.jpg"></center>

<p>
The seemingly unpoetic names <tt>f1</tt> and <tt>f2</tt> serve to remind you of two
things: <tt>f1</tt> (the mapper) is used before <tt>f2</tt> (the reducer), and <tt>
f1</tt> takes one argument while <tt>f2</tt> takes two arguments (just like the
functions used with ordinary <tt>map</tt> and <tt>accumulate</tt> respectively).

<p><pre>mapper: kv-pair |&rarr; list-of-kv-pairs
reducer: value,partial-result |&rarr; result</pre>

<b>All data are in the form of key-value pairs.</b> Ordinary <tt>map</tt> doesn't
care what the elements of the data list argument are, but <tt>mapreduce</tt>
works only with data each of which is a key-value pair.  In the Scheme
interface to the distributed filesystem, a file is a stream.  Every element of
the file stream represents one line of the file, using a key-value pair whose
key is the filename and whose value is a list of words, representing the text
of the line.  The <tt>cdr</tt> of a file stream is a promise to ask the
distributed filesystem for the next line.

<p>
<b>Each processor runs a separate <tt>stream-map</tt>.</b>  The overlapping squares
at the left of the mapreduce picture represent an entire stream.  (How is a
large distributed file divided among map processes?  It doesn't really matter,
as far as the <tt>mapreduce</tt> user is concerned; <tt>mapreduce</tt> tries to do
it as efficiently as possible given the number of processes and the location
of the data in the filesystem.)  The entire stream is the input to a <tt>map</tt>
process; each element of the stream (a kv-pair) is the input to your mapper
function <tt>f1</tt>.

<p>
<b>For each key-value pair in the input stream, the mapper returns a
<u>list</u> of key-value pairs.</b> In the simplest case, each of these lists
will have one element; the code will look something like

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (list (make-kv-pair ... ...)))
</pre></tt>

<p>
The interface requires that you return a list to allow for the non-simplest
cases: (1) Each input key-value pair may give rise to more than one output
key-value pair.  For example, you may want an output key-value pair for each
<i>word</i> of the input file, whereas the input key-value pair represents an
entire line:

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (map (lambda (wd) (make-kv-pair ... ...))
       (kv-value input-kv-pair)))
</pre></tt>

<p>
(2) There are <i>three</i> commonly used higher order functions
for sequential data, <tt>map</tt>, <tt>accumulate</tt>/<tt>reduce</tt>, and <tt>
filter</tt>.  The way <tt>mapreduce</tt> handles the sort of problem for which <tt>
filter</tt> would ordinarily be used is to allow a mapper to return an empty list
if this particular key-value pair shouldn't contribute to the result:

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (if ...
      (list input-kv-pair)
      '()))
</pre></tt>

<p>
Of course it's possible to write mapper functions that combine these three
patterns for more complicated tasks.

<p>
The keys in the kv-pairs returned by the mapper need not be the same as the
key in the input kv-pair.

<p>
<b>Instead of one big accumulation, there's a separate accumulation of
values for each key.</b> The non-parallel computation in the left half of the
picture has two steps, a <tt>map</tt> and an <tt>accumulate</tt>.  But the <tt>
mapreduce</tt> computation has <i>three</i> steps; the middle step sorts all the
key-value pairs produced by all the mapper processes by their keys, and
combines all the kv-pairs with the same key into a single aggregate structure,
which is then used as the input to a <tt>reduce</tt> process.

<p>
<i>This is why the use of key-value pairs is important!</i> If the data had
no such structure imposed on them, there would be no way for us to tell
<tt>mapreduce</tt> which data should be combined in each reduction.

<p>
Although it's shown as one big box, the sort is also done in parallel; it's a
"bucket sort," in which each <tt>map</tt> process is responsible for sending
each of its output kv-pairs to the proper <tt>reduce</tt> process.  (Don't be
confused; your mapper function doesn't have to do that.  The <tt>mapreduce</tt>
program takes care of it.)

<p>
Since all the data seen by a single <tt>reduce</tt> process have the same key,
the reducer doesn't deal with keys at all.  This is important because it
allows us to use simple reducer functions such as <tt>+</tt>, <tt>*</tt>, <tt>max</tt>,
etc.  The Scheme interface to <tt>mapreduce</tt> recognizes the special cases of
<tt>cons</tt> and <tt>cons-stream</tt> as reducers and does what you intend, even
though it wouldn't actually work without this special handling, both because
<tt>cons-stream</tt> is a special form and because the iterative implementation
of <tt>mapreduce</tt> would do the combining in the wrong order.

<p>
In the underlying <tt>mapreduce</tt> software, each <tt>reduce</tt> process leaves
its results in a separate file, stored on the particular processor that ran
the process.  But the Scheme interface to <tt>mapreduce</tt> returns a single
value, a stream that effectively merges the results from all the
<tt>reduce</tt> processes.

<p>

<p>
<b>Running mapreduce:</b> The <tt>mapreduce</tt> function is not available on the
standard lab machines.  You must connect to the machine that controls the
parallel cluster.  To do this, from the Unix shell you say this:

<p>
<tt>  <pre>ssh icluster1.eecs.berkeley.edu
</pre></tt>

<p>
If you're at home, rather than in the lab, you'll have to provide your class
login to the <tt>ssh</tt> command:

<p>
<tt>  <pre>ssh cs61as-XY@icluster1.eecs.berkeley.edu
</pre></tt>

<p>
replacing <tt>XY</tt> above with your login account.  <tt>Ssh</tt> will ask for your
password, which is the same on the parallel cluster as for your regular class
account.  Once you are logged into <tt>icluster1</tt>, you can run <tt>stk</tt> as
usual, but <tt>mapreduce</tt> will be available:

<p>
<tt>  <pre>(mapreduce mapper reducer reducer-base-case filename-or-special-stream)
</pre></tt>

<p>
The first three arguments are the mapper function for the <tt>map</tt> phase,
and the reducer function and starting value for the <tt>reduce</tt> phase.  The
last argument is the data input to the <tt>map</tt>, but it is restricted to
be either a distributed filesystem folder, which must be one of these:

<p>
<tt>  <pre>"/beatles-songs"         <span class="roman">This one is small and has all Beatles song names</span><!--hbox-->
"/gutenberg/shakespeare" <span class="roman">The collected works of William Shakespeare</span><!--hbox-->
"/gutenberg/dickens"     <span class="roman">The collected works of Charles Dickens</span><!--hbox-->
"/sample-emails"         <span class="roman">Some sample email data for the homework</span><!--hbox-->
"/large-emails"          <span class="roman">A much larger sample email dataset. Use this only</span><!--hbox-->
                         <span class="roman">if you're willing to wait a while.</span><!--hbox-->
</pre></tt>

<p>
(the quotation marks above are required), or the stream returned by an earlier
call to <tt>mapreduce</tt>.  (Streams you make yourself with <tt>cons-stream</tt>,
etc., can't be used.)  Some problems are solved with two <tt>mapreduce</tt>
passes, like this:

<p>
<tt>  <pre>(define intermediate-result (mapreduce ...))
(mapreduce ... intermediate-result)
</pre></tt>

<p>
(Yes, you could just use one <tt>mapreduce</tt> call directly as the argument to
the second <tt>mapreduce</tt> call, but in practice you'll want to use
<tt>show-stream</tt> to examine the intermediate result first, to make sure the
first call did what you expect.)

<p>
Here's a sample.  We provide a file of key-value pairs in which the key is
the name of a Beatles album and the value is the name of a song on that album.
Suppose we want to know how many times each word appears in the name of a
song:

<p>
<tt>  <pre>(define (wordcount-mapper document-line-kv-pair)
  (map (lambda (wd-in-line) (make-kv-pair wd-in-line 1))
       (kv-value document-line-kv-pair)))

(define wordcounts (mapreduce wordcount-mapper + 0 "/beatles-songs"))

&#62; (ss wordcounts)
</pre></tt>

<p>
The argument to <tt>wordcount-mapper</tt> will be a key-value pair whose key is
an album name, and whose value is a song name.  (In other examples, the key
will be a filename, such as the name of a play by Shakespeare, and the value
will be a line from the play.)  We're interested only in the song names, so
there's no call to <tt>kv-key</tt> in the procedure.  For each song name, we
generate a list of key-value pairs in which the key is a word in the name and
the value is 1.  This may seem silly, having the same value in every pair,
but it means that in the <tt>reduce</tt> stage we can just use <tt>+</tt> as the
reducer, and it'll add up all the occurrences of each word.

<p>
You'll find the running time disappointing in this example; since the number
of Beatles songs is pretty small, the same computation could be done faster on
a single machine.  This is because there is a significant setup time both
for <tt>mapreduce</tt> itself and for the <tt>stk</tt> interface.  Since your mapper
and reducer functions have to work when run on parallel machines, your Scheme
environment must be shipped over to each of those machines before the
computation begins, so that bindings are available for any free references in
your procedures.  It's only for large amounts of data (or long computations
that aren't data-driven, such as calculating a trillion digits of <font face="symbol">p</font
>, but
<tt>mapreduce</tt> isn't really appropriate for those examples) that parallelism
pays off.

<p>
By the way, if you want to examine the input file, you can't just say

<p>
<tt>  <pre>(ss "/beatles-songs")	; NO
</pre></tt>

<p>
because a distributed filename isn't a stream, even though the file itself
is (when viewed by the <tt>stk</tt> interface to <tt>mapreduce</tt>) a stream.
These filenames only work as arguments to <tt>mapreduce</tt> itself.  But we
can use <tt>mapreduce</tt> to examine the file by applying null transformations
in the map and reduce stages:

<p>
<tt>  <pre>(ss (mapreduce list cons-stream the-empty-stream "/beatles-songs"))
</pre></tt>

<p>
The mapper function is <tt>list</tt> because the mapper must always return a
list of key-value pairs; in this case, <tt>map</tt> will call <tt>list</tt> with
one argument and so it'll return a list of length one.

<p>
Now we'd like to find the most commonly used word in Beatle song titles.
There are few enough words so that we could really do this on one processor,
but as an exercise in parallelism we'll do it partly in parallel.  The trick
is to have each reduce process find the most common word starting with a
particular letter.  Then we'll have 26 candidates from which to choose the
absolutely most common word on one processor.

<p>
<tt>  <pre>(define (find-max-mapper kv-pair)
  (list (make-kv-pair (first (kv-key kv-pair))
		      kv-pair)))

(define (find-max-reducer current so-far)
  (if (&#62; (kv-value current) (kv-value so-far))
      current
      so-far))

(define frequent (mapreduce find-max-mapper find-max-reducer
			    (make-kv-pair 'foo 0) wordcounts))

&#62; (ss frequent)

&#62; (stream-accumulate find-max-reducer (make-kv-pair 'foo 0)
		     (stream-map kv-value frequent))
</pre></tt>

<p>
This is a little tricky.  In the <tt>wordcounts</tt> stream, each key-value
pair has a word as the key, and the count for that word as the value:
<tt>(back&nbsp;.&nbsp;3)</tt>.  The mapper transforms this into a key-value pair in
which the key is the first letter of the word, and the value is <i>the
entire input key-value pair</i>: <tt>(b&nbsp;.&nbsp;(back&nbsp;.&nbsp;3))</tt>.  Each <tt>reduce</tt>
process gets all the pairs with a particular key, i.e., all the ones with
the same first letter of the word.  The reducer sees only the values from
those pairs, but each value is itself a key-value pair!  That's why the
reducer has to compare the <tt>kv-value</tt> of its two arguments.

<p>
As another example, here's a way to count the total number of lines in all
of Shakespeare's plays:

<p>
<tt>  <pre>(define will (mapreduce (lambda (kv-pair) (list (make-kv-pair 'line 1)))
			+ 0 "/gutenberg/shakespeare"))
</pre></tt>

<p>
For each line in Shakespeare, we make exactly the same pair <tt>(line&nbsp;.&nbsp;1)</tt>.
Then, in the <tt>reduce</tt> stage, all the ones in all those pairs are added.
But this is actually a bad example!  Since all the keys are the same (the
word <tt>line</tt>), only one <tt>reduce</tt> process is run, so the counting isn't
done in parallel.  A better way would be to count each play separately, then
add those results if desired.  You'll do that in lab.

<p>